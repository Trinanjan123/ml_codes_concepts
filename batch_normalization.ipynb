{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef1b25bb-bed0-446f-a940-b43bc2e654a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff3fa9-5ae2-4f6a-80f8-fa9d5a941fdb",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237da72-a6e5-4dd8-85a2-def859920d64",
   "metadata": {},
   "source": [
    "- Helps in training deep neural networks reliably (and fast) since the distribution from one layer to another remains same (no effort is spent to learn the new distribution)\n",
    "- Mean and std are perfectly differentiable functions and hence it can be used\n",
    "- We want the pre-activation values to be roughly gaussian only at initialization\n",
    "- We don't want the pre-activations to be forced to be gaussians always, we would like the neural net to be able to move these distributions around to make it more diffuse, more shape, make some neurons more trigger happy or less trigger happy\n",
    "- We want the backpropagation algorithm to tell us how the distribution should move around\n",
    "- The above is achieved by doing scale and shift\n",
    "- The examples in a batch are coupled mathematically in the forward and backward pass of a neural net\n",
    "- Hence the pre-activations and the logits are functions of all examples in the batch\n",
    "- The pre-activation values change slightly depending on the other examples present in the batch and it will jitter\n",
    "- This jittering has a regularization effect\n",
    "- Stabilized training\n",
    "- Allows for suboptimal initializations\n",
    "- Even outs the slope of the loss function resulting in faster optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee417408-dbca-4ea5-917f-09f9b34b1bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm1d:\n",
    "\n",
    "    def __init__(self, dim, momentum=0.1, eps=1e-5):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # params trained with backprop\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # buffers (trained with a running 'momentum' update)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # forward pass\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdims=True) # batch mean\n",
    "            xvar = x.var(0, keepdims=True) # batch std\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma*xhat + self.beta\n",
    "        # update the buffers\n",
    "        if self.training:\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return (self.gamma, self.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f40a3-2063-438e-8870-f85fec0c586a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
